nohup: ignoring input
Found cached dataset json (/root/.cache/huggingface/datasets/json/default-3e8414767b261252/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
Found cached dataset json (/root/.cache/huggingface/datasets/json/default-9de175619a16880a/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)
Dataset({
    features: ['question', 'answer', 'full_prompt', 'inference_prompt'],
    num_rows: 7473
})
Dataset({
    features: ['question', 'answer', 'full_prompt', 'inference_prompt'],
    num_rows: 1319
})
{'question': 'Julie is reading a 120-page book. Yesterday, she was able to read 12 pages and today, she read twice as many pages as yesterday. If she wants to read half of the remaining pages tomorrow, how many pages should she read?', 'answer': 'Maila read 12 x 2 = <<12*2=24>>24 pages today.\nSo she was able to read a total of 12 + 24 = <<12+24=36>>36 pages since yesterday.\nThere are 120 - 36 = <<120-36=84>>84 pages left to be read.\nSince she wants to read half of the remaining pages tomorrow, then she should read 84/2 = <<84/2=42>>42 pages.\n#### 42', 'full_prompt': '\nGiven the following example:\nProblem:\nJoy can read 8 pages of a book in 20 minutes. How many hours will it take her to read 120 pages?\n\nAnswer:\nIn one hour, there are 3 sets of 20 minutes.\nSo, Joy can read 8 x 3 = <<8*3=24>>24 pages in an hour.\nIt will take her 120/24 = <<120/24=5>>5 hours to read 120 pages.\n#### 5\n\nSolve the following math problem thinking step-by-step:\nProblem:\nJulie is reading a 120-page book. Yesterday, she was able to read 12 pages and today, she read twice as many pages as yesterday. If she wants to read half of the remaining pages tomorrow, how many pages should she read?\n\nAnswer:\nMaila read 12 x 2 = <<12*2=24>>24 pages today.\nSo she was able to read a total of 12 + 24 = <<12+24=36>>36 pages since yesterday.\nThere are 120 - 36 = <<120-36=84>>84 pages left to be read.\nSince she wants to read half of the remaining pages tomorrow, then she should read 84/2 = <<84/2=42>>42 pages.\n#### 42', 'inference_prompt': '\nGiven the following example:\nProblem:\nJoy can read 8 pages of a book in 20 minutes. How many hours will it take her to read 120 pages?\n\nAnswer:\nIn one hour, there are 3 sets of 20 minutes.\nSo, Joy can read 8 x 3 = <<8*3=24>>24 pages in an hour.\nIt will take her 120/24 = <<120/24=5>>5 hours to read 120 pages.\n#### 5\n\nSolve the following math problem thinking step-by-step:\nProblem:\nJulie is reading a 120-page book. Yesterday, she was able to read 12 pages and today, she read twice as many pages as yesterday. If she wants to read half of the remaining pages tomorrow, how many pages should she read?\n\nAnswer:'}
╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:261 │
│ in hf_raise_for_status                                                       │
│                                                                              │
│   258 │   </Tip>                                                             │
│   259 │   """                                                                │
│   260 │   try:                                                               │
│ ❱ 261 │   │   response.raise_for_status()                                    │
│   262 │   except HTTPError as e:                                             │
│   263 │   │   error_code = response.headers.get("X-Error-Code")              │
│   264                                                                        │
│                                                                              │
│ /opt/conda/lib/python3.10/site-packages/requests/models.py:1021 in           │
│ raise_for_status                                                             │
│                                                                              │
│   1018 │   │   │   )                                                         │
│   1019 │   │                                                                 │
│   1020 │   │   if http_error_msg:                                            │
│ ❱ 1021 │   │   │   raise HTTPError(http_error_msg, response=self)            │
│   1022 │                                                                     │
│   1023 │   def close(self):                                                  │
│   1024 │   │   """Releases the connection back to the pool. Once this method │
╰──────────────────────────────────────────────────────────────────────────────╯
HTTPError: 401 Client Error: Unauthorized for url: 
https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json

The above exception was the direct cause of the following exception:

╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:429 in     │
│ cached_file                                                                  │
│                                                                              │
│    426 │   user_agent = http_user_agent(user_agent)                          │
│    427 │   try:                                                              │
│    428 │   │   # Load from URL or cache if already cached                    │
│ ❱  429 │   │   resolved_file = hf_hub_download(                              │
│    430 │   │   │   path_or_repo_id,                                          │
│    431 │   │   │   filename,                                                 │
│    432 │   │   │   subfolder=None if len(subfolder) == 0 else subfolder,     │
│                                                                              │
│ /opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py │
│ :118 in _inner_fn                                                            │
│                                                                              │
│   115 │   │   if check_use_auth_token:                                       │
│   116 │   │   │   kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.__na │
│   117 │   │                                                                  │
│ ❱ 118 │   │   return fn(*args, **kwargs)                                     │
│   119 │                                                                      │
│   120 │   return _inner_fn  # type: ignore                                   │
│   121                                                                        │
│                                                                              │
│ /opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:119 │
│ 5 in hf_hub_download                                                         │
│                                                                              │
│   1192 │   if not local_files_only:                                          │
│   1193 │   │   try:                                                          │
│   1194 │   │   │   try:                                                      │
│ ❱ 1195 │   │   │   │   metadata = get_hf_file_metadata(                      │
│   1196 │   │   │   │   │   url=url,                                          │
│   1197 │   │   │   │   │   token=token,                                      │
│   1198 │   │   │   │   │   proxies=proxies,                                  │
│                                                                              │
│ /opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py │
│ :118 in _inner_fn                                                            │
│                                                                              │
│   115 │   │   if check_use_auth_token:                                       │
│   116 │   │   │   kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.__na │
│   117 │   │                                                                  │
│ ❱ 118 │   │   return fn(*args, **kwargs)                                     │
│   119 │                                                                      │
│   120 │   return _inner_fn  # type: ignore                                   │
│   121                                                                        │
│                                                                              │
│ /opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:154 │
│ 1 in get_hf_file_metadata                                                    │
│                                                                              │
│   1538 │   │   proxies=proxies,                                              │
│   1539 │   │   timeout=timeout,                                              │
│   1540 │   )                                                                 │
│ ❱ 1541 │   hf_raise_for_status(r)                                            │
│   1542 │                                                                     │
│   1543 │   # Return                                                          │
│   1544 │   return HfFileMetadata(                                            │
│                                                                              │
│ /opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:277 │
│ in hf_raise_for_status                                                       │
│                                                                              │
│   274 │   │   │   message = (                                                │
│   275 │   │   │   │   f"{response.status_code} Client Error." + "\n\n" + f"C │
│   276 │   │   │   )                                                          │
│ ❱ 277 │   │   │   raise GatedRepoError(message, response) from e             │
│   278 │   │                                                                  │
│   279 │   │   elif error_code == "RepoNotFound" or response.status_code == 4 │
│   280 │   │   │   # 401 is misleading as it is returned for:                 │
╰──────────────────────────────────────────────────────────────────────────────╯
GatedRepoError: 401 Client Error. (Request ID: 
Root=1-651c5532-7cf4660c14fa067077750a3e;2e556966-0bfb-4583-9c2d-b98a1525af9e)

Cannot access gated repo for url 
https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.
Repo model meta-llama/Llama-2-7b-hf is gated. You must be authenticated to 
access it.

The above exception was the direct cause of the following exception:

╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /home/sam/finetune-llm-for-rag/training-code/script-version.py:101 in        │
│ <module>                                                                     │
│                                                                              │
│    98                                                                        │
│    99 # %%                                                                   │
│   100 base_model = "meta-llama/Llama-2-7b-hf"                                │
│ ❱ 101 model = AutoModelForCausalLM.from_pretrained(                          │
│   102 │   base_model,                                                        │
│   103 │   load_in_8bit=True,                                                 │
│   104 │   torch_dtype=torch.float16,                                         │
│                                                                              │
│ /opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factor │
│ y.py:494 in from_pretrained                                                  │
│                                                                              │
│   491 │   │   if commit_hash is None:                                        │
│   492 │   │   │   if not isinstance(config, PretrainedConfig):               │
│   493 │   │   │   │   # We make a call to the config file first (which may b │
│ ❱ 494 │   │   │   │   resolved_config_file = cached_file(                    │
│   495 │   │   │   │   │   pretrained_model_name_or_path,                     │
│   496 │   │   │   │   │   CONFIG_NAME,                                       │
│   497 │   │   │   │   │   _raise_exceptions_for_missing_entries=False,       │
│                                                                              │
│ /opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:444 in     │
│ cached_file                                                                  │
│                                                                              │
│    441 │   │   │   local_files_only=local_files_only,                        │
│    442 │   │   )                                                             │
│    443 │   except GatedRepoError as e:                                       │
│ ❱  444 │   │   raise EnvironmentError(                                       │
│    445 │   │   │   "You are trying to access a gated repo.\nMake sure to req │
│    446 │   │   │   f"https://huggingface.co/{path_or_repo_id} and pass a tok │
│    447 │   │   │   "by logging in with `huggingface-cli login` or by passing │
╰──────────────────────────────────────────────────────────────────────────────╯
OSError: You are trying to access a gated repo.
Make sure to request access at https://huggingface.co/meta-llama/Llama-2-7b-hf 
and pass a token having permission to this repo either by logging in with 
`huggingface-cli login` or by passing `token=<your_token>`.
