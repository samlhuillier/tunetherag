{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-451703fd23ff8620/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-941f6b50a392c31c/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['db_id', 'context', 'question', 'answer', 'full_prompt', 'inference_prompt'],\n",
      "    num_rows: 3961\n",
      "})\n",
      "Dataset({\n",
      "    features: ['db_id', 'context', 'question', 'answer', 'full_prompt', 'inference_prompt'],\n",
      "    num_rows: 568\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# (If you have import errors, try restarting your Jupyter kernel)\n",
    "# \n",
    "\n",
    "# %% [markdown]\n",
    "# ### Load dataset\n",
    "# \n",
    "\n",
    "# %%\n",
    "from datasets import load_dataset\n",
    "from datasets import load_dataset\n",
    "train_dataset = load_dataset('json', data_files='/root/finetune-llm-for-rag/datasets/sql/MiniLM-L6/sql-create-context-spider-intersect-train-with-prompts.jsonl', split='train')\n",
    "eval_dataset = load_dataset('json', data_files='/root/finetune-llm-for-rag/datasets/sql/MiniLM-L6/sql-create-context-spider-intersect-validation-with-prompts.jsonl', split='train')\n",
    "print(train_dataset)\n",
    "print(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-hf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_eos_token = True\n",
    "tokenizer.pad_token_id = 0\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# %% [markdown]\n",
    "# Setup the tokenize function to make labels and input_ids the same. This is basically what [self-supervised fine-tuning](https://neptune.ai/blog/self-supervised-learning) is:\n",
    "\n",
    "# %%\n",
    "def tokenize(prompt):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    # print(\"result is: \", result)\n",
    "    print(\"length of input ids: \", len(result[\"input_ids\"]))\n",
    "\n",
    "    # \"self-supervised learning\" means the labels are also the inputs:\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result\n",
    "\n",
    "# %% [markdown]\n",
    "# And run convert each data_point into a prompt that I found online that works quite well:\n",
    "\n",
    "# %%\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = data_point[\"full_prompt\"]\n",
    "    # print(full_prompt)\n",
    "    return tokenize(full_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ad827aa9e647a28dedc5cddc46980b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/568 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of input ids:  426\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  469\n",
      "length of input ids:  512\n",
      "length of input ids:  505\n",
      "length of input ids:  484\n",
      "length of input ids:  481\n",
      "length of input ids:  512\n",
      "length of input ids:  452\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  447\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  511\n",
      "length of input ids:  434\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  422\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  499\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  466\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  400\n",
      "length of input ids:  480\n",
      "length of input ids:  443\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  501\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  489\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  488\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  488\n",
      "length of input ids:  371\n",
      "length of input ids:  512\n",
      "length of input ids:  450\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  471\n",
      "length of input ids:  362\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  446\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  430\n",
      "length of input ids:  452\n",
      "length of input ids:  419\n",
      "length of input ids:  377\n",
      "length of input ids:  406\n",
      "length of input ids:  512\n",
      "length of input ids:  454\n",
      "length of input ids:  462\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  398\n",
      "length of input ids:  396\n",
      "length of input ids:  464\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  508\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  479\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  461\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  483\n",
      "length of input ids:  502\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  357\n",
      "length of input ids:  477\n",
      "length of input ids:  512\n",
      "length of input ids:  385\n",
      "length of input ids:  484\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  496\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  425\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  478\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  433\n",
      "length of input ids:  512\n",
      "length of input ids:  505\n",
      "length of input ids:  512\n",
      "length of input ids:  501\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  503\n",
      "length of input ids:  504\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  426\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  457\n",
      "length of input ids:  489\n",
      "length of input ids:  511\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  401\n",
      "length of input ids:  486\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  467\n",
      "length of input ids:  512\n",
      "length of input ids:  489\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  491\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  440\n",
      "length of input ids:  416\n",
      "length of input ids:  444\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  478\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  487\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  459\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  455\n",
      "length of input ids:  483\n",
      "length of input ids:  460\n",
      "length of input ids:  512\n",
      "length of input ids:  446\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  503\n",
      "length of input ids:  512\n",
      "length of input ids:  410\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  505\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  378\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  510\n",
      "length of input ids:  390\n",
      "length of input ids:  386\n",
      "length of input ids:  478\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  413\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  462\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  463\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  391\n",
      "length of input ids:  432\n",
      "length of input ids:  463\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  412\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  469\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  434\n",
      "length of input ids:  428\n",
      "length of input ids:  428\n",
      "length of input ids:  466\n",
      "length of input ids:  449\n",
      "length of input ids:  512\n",
      "length of input ids:  401\n",
      "length of input ids:  389\n",
      "length of input ids:  512\n",
      "length of input ids:  427\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  496\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  479\n",
      "length of input ids:  486\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  468\n",
      "length of input ids:  445\n",
      "length of input ids:  406\n",
      "length of input ids:  412\n",
      "length of input ids:  509\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  423\n",
      "length of input ids:  428\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  417\n",
      "length of input ids:  510\n",
      "length of input ids:  512\n",
      "length of input ids:  474\n",
      "length of input ids:  384\n",
      "length of input ids:  440\n",
      "length of input ids:  496\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  420\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  501\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  505\n",
      "length of input ids:  512\n",
      "length of input ids:  485\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  475\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  490\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  503\n",
      "length of input ids:  477\n",
      "length of input ids:  489\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  474\n",
      "length of input ids:  469\n",
      "length of input ids:  477\n",
      "length of input ids:  446\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  421\n",
      "length of input ids:  438\n",
      "length of input ids:  466\n",
      "length of input ids:  467\n",
      "length of input ids:  332\n",
      "length of input ids:  463\n",
      "length of input ids:  484\n",
      "length of input ids:  512\n",
      "length of input ids:  469\n",
      "length of input ids:  423\n",
      "length of input ids:  432\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  355\n",
      "length of input ids:  512\n",
      "length of input ids:  402\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  472\n",
      "length of input ids:  373\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  477\n",
      "length of input ids:  507\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  481\n",
      "length of input ids:  475\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  507\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  454\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  473\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  504\n",
      "length of input ids:  501\n",
      "length of input ids:  512\n",
      "length of input ids:  476\n",
      "length of input ids:  512\n",
      "length of input ids:  430\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  472\n",
      "length of input ids:  512\n",
      "length of input ids:  495\n",
      "length of input ids:  512\n",
      "length of input ids:  464\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  466\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  362\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n",
      "length of input ids:  512\n"
     ]
    }
   ],
   "source": [
    "# tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ae40aed013146bc883465656cb8aded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-hf\")\n",
    "base_model = \"codellama/CodeLlama-7b-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "# Add EOS token and padding configurations\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.pad_token_id = 0\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# Initialize variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.bos_token)\n",
    "print(tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(prompt):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=True,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "\n",
    "    # \"self-supervised learning\" means the labels are also the inputs:\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized IDs: [1, 15043, 872, 566, 2]\n",
      "Pad Token ID: 0\n",
      "length of input ids:  5\n",
      "Pad token ID is not present in the tokenized sequence.\n"
     ]
    }
   ],
   "source": [
    "tokenizer.add_eos_token = True\n",
    "tokenizer.pad_token_id = 0\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# Tokenize a short prompt with padding and max_length set to 10\n",
    "prompt = \"Hello saudfjapwejsdjfioap  osdifwe89jaf asoidfa ssa p apfsdj oeia spda p apapap aifoias dfda ddoao sss aaooa \"\n",
    "result = tokenizer(\n",
    "    prompt,\n",
    "    truncation=True,\n",
    "    max_length=5,\n",
    "    padding=True,\n",
    "    return_tensors=None,\n",
    ")\n",
    "\n",
    "# Display the tokenized IDs and check for padding token ID\n",
    "print(\"Tokenized IDs:\", result['input_ids'])\n",
    "print(\"Pad Token ID:\", tokenizer.pad_token_id)\n",
    "print(\"length of input ids: \", len(result[\"input_ids\"]))\n",
    "# Check if pad token ID is present in the tokenized sequence\n",
    "if tokenizer.pad_token_id in result['input_ids']:\n",
    "    print(\"Pad token ID is present in the tokenized sequence.\")\n",
    "else:\n",
    "    print(\"Pad token ID is not present in the tokenized sequence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1, 22172, 15358, 2], 'attention_mask': [1, 1, 1, 1], 'labels': [1, 22172, 15358, 2]}\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenize(\"hello mate\")\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1, 22172, 15358,     2]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# Sam's manual tests. We want to just investigate the different properties of tokenizers and what they do.\n",
    "tokenized_text = tokenizer(\"hello mate\", return_tensors=\"pt\") # , padding='max_length', max_length=max_input_tokens+10\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a418c719b9453481c71cf844b8fe90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1e2e82ea175448ea0c032612877bb6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)fetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c1eca6bece47eb92deadc215c19b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e364b8c57b874d9bbf4a150fdd55cd03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c50f111a6d3412c812bf11cae9bf1a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37c448724a6142d282a637f32c2a47c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9639a70cfabc4571bc91ab0fa9e59bef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = \"meta-llama/Llama-2-7b-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tokens:  1003\n",
      "input tokens:  1333\n",
      "input tokens:  1663\n",
      "input tokens:  1993\n",
      "input tokens:  2323\n",
      "input tokens:  2653\n",
      "input tokens:  2983\n",
      "input tokens:  3313\n",
      "input tokens:  3643\n",
      "input tokens:  3973\n",
      "input tokens:  4303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (4096). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tokens:  4633\n",
      "input tokens:  4963\n",
      "input tokens:  5293\n",
      "input tokens:  5623\n",
      "input tokens:  5953\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/root/finetune-llm-for-rag/training-code/investigate_truncation.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvast/root/finetune-llm-for-rag/training-code/investigate_truncation.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mif\u001b[39;00m input_tokens \u001b[39m>\u001b[39m max_input_tokens:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvast/root/finetune-llm-for-rag/training-code/investigate_truncation.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     max_input_tokens \u001b[39m=\u001b[39m input_tokens\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bvast/root/finetune-llm-for-rag/training-code/investigate_truncation.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbatch_model_inputs, max_new_tokens\u001b[39m=\u001b[39;49mmax_new_tokens)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvast/root/finetune-llm-for-rag/training-code/investigate_truncation.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m output_tokens \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvast/root/finetune-llm-for-rag/training-code/investigate_truncation.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Check if the last tokens of the input are present in the output\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1652\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1644\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1645\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1646\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1647\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1648\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1649\u001b[0m     )\n\u001b[1;32m   1651\u001b[0m     \u001b[39m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m   1653\u001b[0m         input_ids,\n\u001b[1;32m   1654\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1655\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mlogits_warper,\n\u001b[1;32m   1656\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1657\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1658\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1659\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1660\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1661\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1662\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1663\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1664\u001b[0m     )\n\u001b[1;32m   1666\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1667\u001b[0m     \u001b[39m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1668\u001b[0m     beam_scorer \u001b[39m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1669\u001b[0m         batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1670\u001b[0m         num_beams\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1675\u001b[0m         max_length\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mmax_length,\n\u001b[1;32m   1676\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2734\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2731\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2733\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2734\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2735\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2736\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2737\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2738\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2739\u001b[0m )\n\u001b[1;32m   2741\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2742\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:820\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    817\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m    819\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 820\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m    821\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    822\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    823\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    824\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    825\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    826\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    827\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    828\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    829\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    830\u001b[0m )\n\u001b[1;32m    832\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    833\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpretraining_tp \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:708\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    701\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    702\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    703\u001b[0m         hidden_states,\n\u001b[1;32m    704\u001b[0m         attention_mask,\n\u001b[1;32m    705\u001b[0m         position_ids,\n\u001b[1;32m    706\u001b[0m     )\n\u001b[1;32m    707\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 708\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    709\u001b[0m         hidden_states,\n\u001b[1;32m    710\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    711\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    712\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    713\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    714\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    715\u001b[0m     )\n\u001b[1;32m    717\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    719\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:424\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    421\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    423\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 424\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[1;32m    425\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    426\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    427\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    428\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    429\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    430\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    431\u001b[0m )\n\u001b[1;32m    432\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[1;32m    434\u001b[0m \u001b[39m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:379\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    377\u001b[0m     attn_output \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m([F\u001b[39m.\u001b[39mlinear(attn_output[i], o_proj_slices[i]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpretraining_tp)])\n\u001b[1;32m    378\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mo_proj(attn_output)\n\u001b[1;32m    381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m output_attentions:\n\u001b[1;32m    382\u001b[0m     attn_weights \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:441\u001b[0m, in \u001b[0;36mLinear8bitLt.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m x\u001b[39m.\u001b[39mdtype:\n\u001b[1;32m    439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mto(x\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m--> 441\u001b[0m out \u001b[39m=\u001b[39m bnb\u001b[39m.\u001b[39;49mmatmul(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, bias\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstate)\n\u001b[1;32m    443\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mhas_fp16_weights:\n\u001b[1;32m    444\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mCB \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mCxB \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    445\u001b[0m         \u001b[39m# we converted 8-bit row major to turing/ampere format in the first inference pass\u001b[39;00m\n\u001b[1;32m    446\u001b[0m         \u001b[39m# we no longer need the row-major weight\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:563\u001b[0m, in \u001b[0;36mmatmul\u001b[0;34m(A, B, out, state, threshold, bias)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[39mif\u001b[39;00m threshold \u001b[39m>\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[1;32m    562\u001b[0m     state\u001b[39m.\u001b[39mthreshold \u001b[39m=\u001b[39m threshold\n\u001b[0;32m--> 563\u001b[0m \u001b[39mreturn\u001b[39;00m MatMul8bitLt\u001b[39m.\u001b[39;49mapply(A, B, out, bias, state)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39msetup_context \u001b[39m==\u001b[39m _SingleLevelFunction\u001b[39m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mstaticmethod. For more details, please see \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:327\u001b[0m, in \u001b[0;36mMatMul8bitLt.forward\u001b[0;34m(ctx, A, B, out, bias, state)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(A\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[1;32m    326\u001b[0m     A \u001b[39m=\u001b[39m A\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, A\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m--> 327\u001b[0m CA, CAt, SCA, SCAt, coo_tensorA \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mdouble_quant(A\u001b[39m.\u001b[39;49mto(torch\u001b[39m.\u001b[39;49mfloat16), threshold\u001b[39m=\u001b[39;49mstate\u001b[39m.\u001b[39;49mthreshold)\n\u001b[1;32m    329\u001b[0m \u001b[39mif\u001b[39;00m state\u001b[39m.\u001b[39mthreshold \u001b[39m>\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mand\u001b[39;00m coo_tensorA \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mif\u001b[39;00m state\u001b[39m.\u001b[39mhas_fp16_weights:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/bitsandbytes/functional.py:2016\u001b[0m, in \u001b[0;36mdouble_quant\u001b[0;34m(A, col_stats, row_stats, out_col, out_row, threshold)\u001b[0m\n\u001b[1;32m   2014\u001b[0m is_on_gpu([A, col_stats, row_stats, out_col, out_row])\n\u001b[1;32m   2015\u001b[0m \u001b[39mif\u001b[39;00m threshold \u001b[39m>\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[0;32m-> 2016\u001b[0m     nnz \u001b[39m=\u001b[39m nnz_row_ptr[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m   2017\u001b[0m     \u001b[39mif\u001b[39;00m nnz \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   2018\u001b[0m         coo_tensor \u001b[39m=\u001b[39m coo_zeros(\n\u001b[1;32m   2019\u001b[0m             A\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], A\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], nnz_row_ptr[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mitem(), device\n\u001b[1;32m   2020\u001b[0m         )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "base_text = \"Add more text here to increase the token count. \" * 100\n",
    "\n",
    "max_new_tokens = 5\n",
    "max_input_tokens = 0\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "# Add EOS token and padding configurations\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.pad_token_id = 0\n",
    "tokenizer.padding_side = \"left\"\n",
    "# tokenizer.padding_size = \"left\"\n",
    "while True:\n",
    "    batch_model_inputs = tokenizer(base_text, return_tensors=\"pt\", padding='max_length', max_length=max_input_tokens+10)\n",
    "    batch_model_inputs.to(\"cuda\")\n",
    "    input_tokens = batch_model_inputs['input_ids'].shape[1]\n",
    "    print(\"input tokens: \", input_tokens)\n",
    "    if input_tokens > max_input_tokens:\n",
    "        max_input_tokens = input_tokens\n",
    "    \n",
    "    output = model.generate(**batch_model_inputs, max_new_tokens=max_new_tokens)\n",
    "    output_tokens = output.shape[1]\n",
    "    # Check if the last tokens of the input are present in the output\n",
    "    last_input_tokens = batch_model_inputs['input_ids'][0, -max_new_tokens:].tolist()\n",
    "    if not all(token in output[0, -max_new_tokens*2:].tolist() for token in last_input_tokens):\n",
    "        print(f\"Context limit reached with input tokens: {input_tokens}\")\n",
    "        break\n",
    "\n",
    "    base_text += \" Add more text here to increase the token count. \"*30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
