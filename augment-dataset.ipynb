{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sam/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from embeddings.chroma_funcs import get_closest_entries, generate_knowledge_base_from_hf_dataset\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_rag_examples(examples):\n",
    "    formatted_examples = \"\\n\".join(\n",
    "        f\"\"\"Example {j+1}:\n",
    "### Input:\n",
    "{example[\"question\"]}\n",
    "\n",
    "### Context:\n",
    "{example[\"context\"]}\n",
    "\n",
    "### Response:\n",
    "{example[\"answer\"]}\n",
    "\"\"\" for j, example in enumerate(examples)\n",
    "    )\n",
    "    \n",
    "    return f\"\"\"Given the following examples:\n",
    "{formatted_examples}\"\"\"\n",
    "\n",
    "\n",
    "def generate_rag_sql_prompt(knowledge_base, data_point, n_examples):\n",
    "    results = get_closest_entries(\n",
    "        knowledge_base,\n",
    "        data_point[\"question\"],\n",
    "        \"question\",\n",
    "        n_results=n_examples,\n",
    "    )\n",
    "    rag_datapoints = results[\"metadatas\"][0]\n",
    "    formatted_examples = format_rag_examples(rag_datapoints)\n",
    "    # print(\"formatted examples are: \", formatted_examples)\n",
    "    inference_prompt = f\"\"\"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables. You must output the SQL query that answers the question.\n",
    "\n",
    "{formatted_examples}\n",
    "Please generate the SQL query that answers the following:\n",
    "### Input:\n",
    "{data_point[\"question\"]}\n",
    "\n",
    "### Context:\n",
    "{data_point[\"context\"]}\n",
    "\n",
    "### Response:\"\"\"\n",
    "    full_prompt = f\"{inference_prompt}\\n{data_point['answer']}\"\n",
    "    return full_prompt, inference_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_datapoint = {\"db_id\":\"department_management\",\"context\":\"CREATE TABLE department (creation VARCHAR)\",\"question\":\"In which year were most departments established?\",\"answer\":\"SELECT creation FROM department GROUP BY creation ORDER BY count(*) DESC LIMIT 1\",\"full_prompt\":\"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables. You must output the SQL query that answers the question.\\n\\nGiven the following examples:\\nExample 1:\\n### Input:\\nList the creation year, name and budget of each department.\\n\\n### Context:\\nCREATE TABLE department (creation VARCHAR, name VARCHAR, budget_in_billions VARCHAR)\\n\\n### Response:\\nSELECT creation ,  name ,  budget_in_billions FROM department\\n\\nExample 2:\\n### Input:\\nlist names of all departments ordered by their names.\\n\\n### Context:\\nCREATE TABLE department (dept_name VARCHAR)\\n\\n### Response:\\nSELECT dept_name FROM department ORDER BY dept_name\\n\\nExample 3:\\n### Input:\\nWhat are the distinct creation years of the departments managed by a secretary born in state 'Alabama'?\\n\\n### Context:\\nCREATE TABLE department (creation VARCHAR, department_id VARCHAR); CREATE TABLE management (department_id VARCHAR, head_id VARCHAR); CREATE TABLE head (head_id VARCHAR, born_state VARCHAR)\\n\\n### Response:\\nSELECT DISTINCT T1.creation FROM department AS T1 JOIN management AS T2 ON T1.department_id  =  T2.department_id JOIN head AS T3 ON T2.head_id  =  T3.head_id WHERE T3.born_state  =  'Alabama'\\n\\nExample 4:\\n### Input:\\nWhat is the number of departments in Division \\\"AS\\\"?\\n\\n### Context:\\nCREATE TABLE DEPARTMENT (Division VARCHAR)\\n\\n### Response:\\nSELECT count(*) FROM DEPARTMENT WHERE Division  =  \\\"AS\\\"\\n\\nExample 5:\\n### Input:\\nFind the names of the top 3 departments that provide the largest amount of courses?\\n\\n### Context:\\nCREATE TABLE course (dept_name VARCHAR)\\n\\n### Response:\\nSELECT dept_name FROM course GROUP BY dept_name ORDER BY count(*) DESC LIMIT 3\\n\\nPlease generate the SQL query that answers the following:\\n### Input:\\nIn which year were most departments established?\\n\\n### Context:\\nCREATE TABLE department (creation VARCHAR)\\n\\n### Response:\\nSELECT creation FROM department GROUP BY creation ORDER BY count(*) DESC LIMIT 1\",\"inference_prompt\":\"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables. You must output the SQL query that answers the question.\\n\\nGiven the following examples:\\nExample 1:\\n### Input:\\nList the creation year, name and budget of each department.\\n\\n### Context:\\nCREATE TABLE department (creation VARCHAR, name VARCHAR, budget_in_billions VARCHAR)\\n\\n### Response:\\nSELECT creation ,  name ,  budget_in_billions FROM department\\n\\nExample 2:\\n### Input:\\nlist names of all departments ordered by their names.\\n\\n### Context:\\nCREATE TABLE department (dept_name VARCHAR)\\n\\n### Response:\\nSELECT dept_name FROM department ORDER BY dept_name\\n\\nExample 3:\\n### Input:\\nWhat are the distinct creation years of the departments managed by a secretary born in state 'Alabama'?\\n\\n### Context:\\nCREATE TABLE department (creation VARCHAR, department_id VARCHAR); CREATE TABLE management (department_id VARCHAR, head_id VARCHAR); CREATE TABLE head (head_id VARCHAR, born_state VARCHAR)\\n\\n### Response:\\nSELECT DISTINCT T1.creation FROM department AS T1 JOIN management AS T2 ON T1.department_id  =  T2.department_id JOIN head AS T3 ON T2.head_id  =  T3.head_id WHERE T3.born_state  =  'Alabama'\\n\\nExample 4:\\n### Input:\\nWhat is the number of departments in Division \\\"AS\\\"?\\n\\n### Context:\\nCREATE TABLE DEPARTMENT (Division VARCHAR)\\n\\n### Response:\\nSELECT count(*) FROM DEPARTMENT WHERE Division  =  \\\"AS\\\"\\n\\nExample 5:\\n### Input:\\nFind the names of the top 3 departments that provide the largest amount of courses?\\n\\n### Context:\\nCREATE TABLE course (dept_name VARCHAR)\\n\\n### Response:\\nSELECT dept_name FROM course GROUP BY dept_name ORDER BY count(*) DESC LIMIT 3\\n\\nPlease generate the SQL query that answers the following:\\n### Input:\\nIn which year were most departments established?\\n\\n### Context:\\nCREATE TABLE department (creation VARCHAR)\\n\\n### Response:\"}\n",
    "# full_prompt, inference_prompt = generate_rag_sql_prompt(train_datapoint)\n",
    "# full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prompt_features(example, knowledge_base, n_examples):\n",
    "    # Add your logic to generate the extra feature here\n",
    "    full_prompt, inference_prompt = generate_rag_sql_prompt(knowledge_base, example, n_examples)\n",
    "    example['full_prompt'] = full_prompt\n",
    "    example['inference_prompt'] = inference_prompt\n",
    "    return example\n",
    "\n",
    "def augment_dataset_with_prompts(dataset_name, knowledge_base, n_examples=5):\n",
    "    # Load the dataset without specifying a split\n",
    "    dataset_dict = load_dataset(dataset_name)\n",
    "\n",
    "    # Iterate over each split in the loaded dataset\n",
    "    for split, dataset in dataset_dict.items():\n",
    "        dataset = dataset.map(\n",
    "            lambda example: add_prompt_features(example, knowledge_base, n_examples=n_examples),\n",
    "        )\n",
    "\n",
    "        # Generate filename based on dataset name and split\n",
    "        filename = f\"{dataset_name.replace('/', '-')}-{split}-with-prompts.jsonl\"\n",
    "        \n",
    "        # Save the dataset as a JSON file\n",
    "        dataset.to_json(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function augment_dataset_with_prompts.<locals>.<lambda> at 0x7f99981fae60> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Map:   7%|â–‹         | 258/3961 [00:23<05:43, 10.78 examples/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/sam/Desktop/finetune-llm-for-rag/augment-dataset.ipynb Cell 5\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sam/Desktop/finetune-llm-for-rag/augment-dataset.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m dataset_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msamlhuillier/sql-create-context-spider-intersect\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sam/Desktop/finetune-llm-for-rag/augment-dataset.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m knowledge_base \u001b[39m=\u001b[39m generate_knowledge_base_from_hf_dataset(dataset_name, \u001b[39m\"\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sam/Desktop/finetune-llm-for-rag/augment-dataset.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m augment_dataset_with_prompts(dataset_name, knowledge_base, n_examples\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32m/Users/sam/Desktop/finetune-llm-for-rag/augment-dataset.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sam/Desktop/finetune-llm-for-rag/augment-dataset.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Iterate over each split in the loaded dataset\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sam/Desktop/finetune-llm-for-rag/augment-dataset.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfor\u001b[39;00m split, dataset \u001b[39min\u001b[39;00m dataset_dict\u001b[39m.\u001b[39mitems():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sam/Desktop/finetune-llm-for-rag/augment-dataset.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mmap(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sam/Desktop/finetune-llm-for-rag/augment-dataset.ipynb#X11sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         \u001b[39mlambda\u001b[39;49;00m example: add_prompt_features(example, knowledge_base, n_examples\u001b[39m=\u001b[39;49mn_examples),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sam/Desktop/finetune-llm-for-rag/augment-dataset.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sam/Desktop/finetune-llm-for-rag/augment-dataset.ipynb#X11sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39m# Generate filename based on dataset name and split\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sam/Desktop/finetune-llm-for-rag/augment-dataset.ipynb#X11sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     filename \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mdataset_name\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m{\u001b[39;00msplit\u001b[39m}\u001b[39;00m\u001b[39m-with-prompts.jsonl\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/datasets/arrow_dataset.py:592\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    591\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 592\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    593\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    594\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m    595\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/datasets/arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    551\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    552\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    553\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    554\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    555\u001b[0m }\n\u001b[1;32m    556\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    558\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    559\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/datasets/arrow_dataset.py:3097\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3090\u001b[0m \u001b[39mif\u001b[39;00m transformed_dataset \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3091\u001b[0m     \u001b[39mwith\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(\n\u001b[1;32m   3092\u001b[0m         disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled(),\n\u001b[1;32m   3093\u001b[0m         unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m examples\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3094\u001b[0m         total\u001b[39m=\u001b[39mpbar_total,\n\u001b[1;32m   3095\u001b[0m         desc\u001b[39m=\u001b[39mdesc \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mMap\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3096\u001b[0m     ) \u001b[39mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3097\u001b[0m         \u001b[39mfor\u001b[39;00m rank, done, content \u001b[39min\u001b[39;00m Dataset\u001b[39m.\u001b[39m_map_single(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3098\u001b[0m             \u001b[39mif\u001b[39;00m done:\n\u001b[1;32m   3099\u001b[0m                 shards_done \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/datasets/arrow_dataset.py:3450\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3448\u001b[0m _time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   3449\u001b[0m \u001b[39mfor\u001b[39;00m i, example \u001b[39min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3450\u001b[0m     example \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(example, i, offset\u001b[39m=\u001b[39;49moffset)\n\u001b[1;32m   3451\u001b[0m     \u001b[39mif\u001b[39;00m update_data:\n\u001b[1;32m   3452\u001b[0m         \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/datasets/arrow_dataset.py:3353\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3351\u001b[0m \u001b[39mif\u001b[39;00m with_rank:\n\u001b[1;32m   3352\u001b[0m     additional_args \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (rank,)\n\u001b[0;32m-> 3353\u001b[0m processed_inputs \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39;49mfn_args, \u001b[39m*\u001b[39;49madditional_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfn_kwargs)\n\u001b[1;32m   3354\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3355\u001b[0m     processed_inputs \u001b[39m=\u001b[39m {\n\u001b[1;32m   3356\u001b[0m         k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mkeys_to_format\n\u001b[1;32m   3357\u001b[0m     }\n",
      "\u001b[1;32m/Users/sam/Desktop/finetune-llm-for-rag/augment-dataset.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sam/Desktop/finetune-llm-for-rag/augment-dataset.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Iterate over each split in the loaded dataset\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sam/Desktop/finetune-llm-for-rag/augment-dataset.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfor\u001b[39;00m split, dataset \u001b[39min\u001b[39;00m dataset_dict\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sam/Desktop/finetune-llm-for-rag/augment-dataset.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mmap(\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sam/Desktop/finetune-llm-for-rag/augment-dataset.ipynb#X11sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         \u001b[39mlambda\u001b[39;00m example: add_prompt_features(example, knowledge_base, n_examples\u001b[39m=\u001b[39;49mn_examples),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sam/Desktop/finetune-llm-for-rag/augment-dataset.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sam/Desktop/finetune-llm-for-rag/augment-dataset.ipynb#X11sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39m# Generate filename based on dataset name and split\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sam/Desktop/finetune-llm-for-rag/augment-dataset.ipynb#X11sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     filename \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mdataset_name\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m{\u001b[39;00msplit\u001b[39m}\u001b[39;00m\u001b[39m-with-prompts.jsonl\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;32m/Users/sam/Desktop/finetune-llm-for-rag/augment-dataset.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sam/Desktop/finetune-llm-for-rag/augment-dataset.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madd_prompt_features\u001b[39m(example, knowledge_base, n_examples):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sam/Desktop/finetune-llm-for-rag/augment-dataset.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m# Add your logic to generate the extra feature here\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sam/Desktop/finetune-llm-for-rag/augment-dataset.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     full_prompt, inference_prompt \u001b[39m=\u001b[39m generate_rag_sql_prompt(knowledge_base, example, n_examples)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sam/Desktop/finetune-llm-for-rag/augment-dataset.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     example[\u001b[39m'\u001b[39m\u001b[39mfull_prompt\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m full_prompt\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sam/Desktop/finetune-llm-for-rag/augment-dataset.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     example[\u001b[39m'\u001b[39m\u001b[39minference_prompt\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m inference_prompt\n",
      "\u001b[1;32m/Users/sam/Desktop/finetune-llm-for-rag/augment-dataset.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sam/Desktop/finetune-llm-for-rag/augment-dataset.ipynb#X11sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_rag_sql_prompt\u001b[39m(knowledge_base, data_point, n_examples):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sam/Desktop/finetune-llm-for-rag/augment-dataset.ipynb#X11sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     results \u001b[39m=\u001b[39m get_closest_entries(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sam/Desktop/finetune-llm-for-rag/augment-dataset.ipynb#X11sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         knowledge_base,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sam/Desktop/finetune-llm-for-rag/augment-dataset.ipynb#X11sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m         data_point[\u001b[39m\"\u001b[39;49m\u001b[39mquestion\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sam/Desktop/finetune-llm-for-rag/augment-dataset.ipynb#X11sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mquestion\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sam/Desktop/finetune-llm-for-rag/augment-dataset.ipynb#X11sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m         n_results\u001b[39m=\u001b[39;49mn_examples,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sam/Desktop/finetune-llm-for-rag/augment-dataset.ipynb#X11sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sam/Desktop/finetune-llm-for-rag/augment-dataset.ipynb#X11sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     rag_datapoints \u001b[39m=\u001b[39m results[\u001b[39m\"\u001b[39m\u001b[39mmetadatas\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sam/Desktop/finetune-llm-for-rag/augment-dataset.ipynb#X11sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     formatted_examples \u001b[39m=\u001b[39m format_rag_examples(rag_datapoints)\n",
      "File \u001b[0;32m~/Desktop/finetune-llm-for-rag/embeddings/chroma_funcs.py:37\u001b[0m, in \u001b[0;36mget_closest_entries\u001b[0;34m(collection, query, embed_feature, n_results)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_closest_entries\u001b[39m(collection, query, embed_feature, n_results\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m):\n\u001b[0;32m---> 37\u001b[0m     results \u001b[39m=\u001b[39m collection\u001b[39m.\u001b[39;49mquery(\n\u001b[1;32m     38\u001b[0m         query_texts\u001b[39m=\u001b[39;49m[query],  \u001b[39m# TODO: look into what multiple queries will do here.\u001b[39;49;00m\n\u001b[1;32m     39\u001b[0m         n_results\u001b[39m=\u001b[39;49mn_results,\n\u001b[1;32m     40\u001b[0m         where\u001b[39m=\u001b[39;49m{embed_feature: {\u001b[39m\"\u001b[39;49m\u001b[39m$ne\u001b[39;49m\u001b[39m\"\u001b[39;49m: query}},  \u001b[39m# exact match is train/val contamination\u001b[39;49;00m\n\u001b[1;32m     41\u001b[0m         \u001b[39m# TODO: test this:\u001b[39;49;00m\n\u001b[1;32m     42\u001b[0m         \u001b[39m# where_document={\"$ne\": query},  # exact match is train/val contamination\u001b[39;49;00m\n\u001b[1;32m     43\u001b[0m     )\n\u001b[1;32m     44\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/chromadb/api/models/Collection.py:210\u001b[0m, in \u001b[0;36mCollection.query\u001b[0;34m(self, query_embeddings, query_texts, n_results, where, where_document, include)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    207\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYou must provide embeddings or a function to compute them\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    208\u001b[0m         )\n\u001b[1;32m    209\u001b[0m     \u001b[39m# We know query texts is not None at this point, cast for the typechecker\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m     query_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_embedding_function(\n\u001b[1;32m    211\u001b[0m         cast(List[Document], query_texts)\n\u001b[1;32m    212\u001b[0m     )\n\u001b[1;32m    214\u001b[0m \u001b[39mif\u001b[39;00m where \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     where \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/chromadb/utils/embedding_functions.py:379\u001b[0m, in \u001b[0;36mONNXMiniLM_L6_V2.__call__\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download_model_if_not_exists()\n\u001b[1;32m    378\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_model_and_tokenizer()\n\u001b[0;32m--> 379\u001b[0m res \u001b[39m=\u001b[39m cast(Embeddings, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(texts)\u001b[39m.\u001b[39mtolist())\n\u001b[1;32m    380\u001b[0m \u001b[39mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/chromadb/utils/embedding_functions.py:328\u001b[0m, in \u001b[0;36mONNXMiniLM_L6_V2._forward\u001b[0;34m(self, documents, batch_size)\u001b[0m\n\u001b[1;32m    319\u001b[0m attention_mask \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([e\u001b[39m.\u001b[39mattention_mask \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m encoded])\n\u001b[1;32m    320\u001b[0m onnx_input \u001b[39m=\u001b[39m {\n\u001b[1;32m    321\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39marray(input_ids, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mint64),\n\u001b[1;32m    322\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39marray(attention_mask, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mint64),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    326\u001b[0m     ),\n\u001b[1;32m    327\u001b[0m }\n\u001b[0;32m--> 328\u001b[0m model_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mrun(\u001b[39mNone\u001b[39;49;00m, onnx_input)\n\u001b[1;32m    329\u001b[0m last_hidden_state \u001b[39m=\u001b[39m model_output[\u001b[39m0\u001b[39m]\n\u001b[1;32m    330\u001b[0m \u001b[39m# Perform mean pooling with attention weighting\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:220\u001b[0m, in \u001b[0;36mSession.run\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    218\u001b[0m     output_names \u001b[39m=\u001b[39m [output\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs_meta]\n\u001b[1;32m    219\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 220\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sess\u001b[39m.\u001b[39;49mrun(output_names, input_feed, run_options)\n\u001b[1;32m    221\u001b[0m \u001b[39mexcept\u001b[39;00m C\u001b[39m.\u001b[39mEPFail \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    222\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_fallback:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# so first we need to generate the knowledge_base\n",
    "dataset_name = \"samlhuillier/sql-create-context-spider-intersect\"\n",
    "knowledge_base = generate_knowledge_base_from_hf_dataset(dataset_name, \"question\")\n",
    "augment_dataset_with_prompts(dataset_name, knowledge_base, n_examples=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables. You must output the SQL query that answers the question.\n",
      "\n",
      "Given the following examples:\n",
      "Example 1:\n",
      "### Input:\n",
      "How many vehicle in total?\n",
      "\n",
      "### Context:\n",
      "CREATE TABLE Vehicles (Id VARCHAR)\n",
      "\n",
      "### Response:\n",
      "SELECT count(*) FROM Vehicles;\n",
      "\n",
      "Example 2:\n",
      "### Input:\n",
      "what is the average number of factories and maximum number of shops for manufacturers that opened before 1990.\n",
      "\n",
      "### Context:\n",
      "CREATE TABLE manufacturer (num_of_shops INTEGER, Num_of_Factories INTEGER, open_year INTEGER)\n",
      "\n",
      "### Response:\n",
      "SELECT max(num_of_shops) ,  avg(Num_of_Factories) FROM manufacturer WHERE open_year  <  1990\n",
      "\n",
      "Please generate the SQL query that answers the following:\n",
      "### Input:\n",
      "What is the average horsepower for all cars produced before 1980 ?\n",
      "\n",
      "### Context:\n",
      "CREATE TABLE cars_data (horsepower INTEGER, year INTEGER)\n",
      "\n",
      "### Response:\n",
      "select avg(horsepower) from cars_data where year  <  1980;\n"
     ]
    }
   ],
   "source": [
    "test_datapoint = {\n",
    "        \"question\": \"What is the average horsepower for all cars produced before 1980 ?\",\n",
    "        \"context\": \"CREATE TABLE cars_data (horsepower INTEGER, year INTEGER)\",\n",
    "        \"answer\": \"select avg(horsepower) from cars_data where year  <  1980;\",\n",
    "        \"db_id\": \"car_1\"\n",
    "    }\n",
    "\n",
    "full_prompt, inference_prompt = generate_rag_sql_prompt(knowledge_base, test_datapoint, n_examples=2)\n",
    "print(full_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
